import os
from openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from supabase import create_client
from flask import current_app
from utils.supabase_client import get_supabase


# === åŠ è½½ä¸­æ–‡å‘é‡æ¨¡å‹ ===
model = SentenceTransformer("BAAI/bge-small-zh")

# ==== æå–å‘é‡ ====
def get_embedding(text):
    return model.encode(text, normalize_embeddings=True).tolist()


# ==== å‘é‡åŒ–æ–‡ä»¶ ====
def embed_txt_file(file_path: str, file_id: int):
    supabase = get_supabase()
    # ğŸ‘‡ è·å– user_id
    file_record = supabase.table("files").select("user_id").eq("id", file_id).single().execute()
    user_id = file_record.data["user_id"]
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
    )
    chunks = splitter.split_text(text)


    for i, chunk in enumerate(chunks):
        try:
            embedding = get_embedding(chunk)
            supabase.table("user_vectors").insert({
                "file_id": file_id,  # âœ… ç”¨å¤–é”®æŒ‡å‘ files è¡¨
                "user_id": user_id,
                "content": chunk,
                "embedding": embedding,
                "metadata": {
                    "chunk_index": i
                }
            }).execute()
            print(f"[âœ…] æ’å…¥ file_id={file_id} ç¬¬ {i + 1} æ®µ")
        except Exception as e:
            print(f"[âŒ] æ’å…¥å¤±è´¥ï¼ˆfile_id={file_id}, chunk={i + 1}ï¼‰ï¼š{e}")
